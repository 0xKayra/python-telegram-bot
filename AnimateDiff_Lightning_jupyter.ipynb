{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers\n",
        "\n",
        "import torch\n",
        "from diffusers import AnimateDiffPipeline, MotionAdapter, EulerDiscreteScheduler\n",
        "from diffusers.utils import export_to_gif\n",
        "from huggingface_hub import hf_hub_download\n",
        "from safetensors.torch import load_file\n",
        "\n",
        "device = \"cuda\"\n",
        "dtype = torch.float16\n",
        "\n",
        "step = 4  # Options: [1,2,4,8]\n",
        "repo = \"ByteDance/AnimateDiff-Lightning\"\n",
        "ckpt = f\"animatediff_lightning_{step}step_diffusers.safetensors\"\n",
        "base = \"emilianJR/epiCRealism\"  # Choose to your favorite base model.\n",
        "\n",
        "adapter = MotionAdapter().to(device, dtype)\n",
        "adapter.load_state_dict(load_file(hf_hub_download(repo ,ckpt), device=device))\n",
        "pipe = AnimateDiffPipeline.from_pretrained(base, motion_adapter=adapter, torch_dtype=dtype).to(device)\n",
        "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\", beta_schedule=\"linear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di-NpHa1HB_G"
      },
      "outputs": [],
      "source": [
        "output = pipe(prompt=\"A girl smiling\", guidance_scale=1.0, num_inference_steps=step)\n",
        "export_to_gif(output.frames[0], \"animation.gif\")\n",
        "from IPython.display import Image\n",
        "Image(open('/content/animation.gif', 'rb').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QDeyzawHB_K"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import imageio\n",
        "\n",
        "def export_to_video(\n",
        "    video_frames: Union[List[np.ndarray], List[PIL.Image.Image]], output_video_path: str = None, fps: int = 10\n",
        ") -> str:\n",
        "    if output_video_path is None:\n",
        "        output_video_path = tempfile.NamedTemporaryFile(suffix=\".mp4\").name\n",
        "    if isinstance(video_frames[0], np.ndarray):\n",
        "        video_frames = [(frame * 255).astype(np.uint8) for frame in video_frames]\n",
        "    elif isinstance(video_frames[0], PIL.Image.Image):\n",
        "        video_frames = [np.array(frame) for frame in video_frames]\n",
        "    writer = imageio.get_writer(output_video_path, fps=fps)\n",
        "    for frame in video_frames:\n",
        "        writer.append_data(frame)\n",
        "    writer.close()\n",
        "    return output_video_path\n",
        "\n",
        "export_to_video(output.frames[0], \"animation.mp4\")\n",
        "from IPython.display import Video\n",
        "Video(\"/content/animation.mp4\", embed=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from diffusers import AnimateDiffSparseControlNetPipeline\n",
        "from diffusers.models import AutoencoderKL, MotionAdapter, SparseControlNetModel\n",
        "from diffusers.schedulers import DPMSolverMultistepScheduler\n",
        "from diffusers.utils import export_to_gif, load_image\n",
        "\n",
        "\n",
        "model_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\n",
        "motion_adapter_id = \"guoyww/animatediff-motion-adapter-v1-5-3\"\n",
        "controlnet_id = \"guoyww/animatediff-sparsectrl-rgb\"\n",
        "lora_adapter_id = \"guoyww/animatediff-motion-lora-v1-5-3\"\n",
        "vae_id = \"stabilityai/sd-vae-ft-mse\"\n",
        "device = \"cuda\"\n",
        "\n",
        "motion_adapter = MotionAdapter.from_pretrained(motion_adapter_id, torch_dtype=torch.float16).to(device)\n",
        "controlnet = SparseControlNetModel.from_pretrained(controlnet_id, torch_dtype=torch.float16).to(device)\n",
        "vae = AutoencoderKL.from_pretrained(vae_id, torch_dtype=torch.float16).to(device)\n",
        "scheduler = DPMSolverMultistepScheduler.from_pretrained(\n",
        "    model_id,\n",
        "    subfolder=\"scheduler\",\n",
        "    beta_schedule=\"linear\",\n",
        "    algorithm_type=\"dpmsolver++\",\n",
        "    use_karras_sigmas=True,\n",
        ")\n",
        "pipe = AnimateDiffSparseControlNetPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    motion_adapter=motion_adapter,\n",
        "    controlnet=controlnet,\n",
        "    vae=vae,\n",
        "    scheduler=scheduler,\n",
        "    torch_dtype=torch.float16,\n",
        ").to(device)\n",
        "pipe.load_lora_weights(lora_adapter_id, adapter_name=\"motion_lora\")\n",
        "\n",
        "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/animatediff-firework.png\")\n",
        "\n",
        "video = pipe(\n",
        "    prompt=\"closeup face photo of man in black clothes, night city street, bokeh, fireworks in background\",\n",
        "    negative_prompt=\"low quality, worst quality\",\n",
        "    num_inference_steps=25,\n",
        "    conditioning_frames=image,\n",
        "    controlnet_frame_indices=[0],\n",
        "    controlnet_conditioning_scale=1.0,\n",
        "    generator=torch.Generator().manual_seed(42),\n",
        ").frames[0]\n",
        "export_to_gif(video, \"output.gif\")"
      ],
      "metadata": {
        "id": "O2h4WA0cHNd8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}